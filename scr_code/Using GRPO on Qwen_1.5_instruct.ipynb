{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9775cbed-eee3-4161-adb7-2dd909362527",
   "metadata": {},
   "source": [
    "## Part 1: Basic Setup and Imports\n",
    "\n",
    "In this first part, we install and import all necessary modules. We also set up our environment by configuring random seeds for reproducibility and initializing environment variables required for experiment tracking. In addition, we install and import libraries that provide optimized transformer attention mechanisms (FlashAttention2) and reporting (Weights and Biases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b652-9c33-449d-a9cf-76d1d13606df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# Basic Python libraries for various operations\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# PyTorch and related libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hugging Face libraries for transformer models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call the function to set random seed for reproducibility\n",
    "set_random_seed(42)\n",
    "\n",
    "# Set environment variables for Weights & Biases (wandb) logging\n",
    "os.environ[\"WANDB_API_KEY\"] = \"0c9120253669190fb57ab7803eecbfd3d4d61b8a\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"GRPO-Qwen-1.5-Instruct-Multi-GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a7c13-c5b7-411c-a2c9-2689c1da0769",
   "metadata": {},
   "source": [
    "## Part 2: Data Formatting and Answer Extraction\n",
    "In this section, we define how our data is formatted and how to extract the answer segments from both the model's output and the dataset. To ensure that the model outputs its response in a consistent format, we define a system prompt. The prompt instructs the model to generate output in an XML-like format containing `<reasoning>` and `<answer>` tags. We then provide two functions:\n",
    "1. **`extract_answer_from_model_output`:** This function takes the model's output text and extracts the content within the `<answer>` tags.\n",
    "2. **`extract_answer_from_dataset`:** This function extracts the expected answer from the GSM8K dataset, which separates the answer using the `\"####\"` delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f9200-62da-480c-a664-2f95bacbd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_answer_from_model_output(text):\n",
    "\n",
    "   # Split on <answer> and take everything after the last occurrence\n",
    "   parts = text.split(\"<answer>\")\n",
    "   if len(parts) < 2:  # No <answer> tag found\n",
    "       return None\n",
    "   last_part = parts[-1]\n",
    "\n",
    "   # Extract content up to </answer>\n",
    "   if \"</answer>\" not in last_part:\n",
    "       return None\n",
    "   answer = last_part.split(\"</answer>\")[0].strip()\n",
    "   return None if answer == \"...\" else answer\n",
    "\n",
    "def extract_answer_from_dataset(text):\n",
    "\n",
    "   if \"####\" not in text:\n",
    "       return None\n",
    "   return text.split(\"####\")[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a8d04-3bbb-498a-8bbe-22f61cf82aba",
   "metadata": {},
   "source": [
    "## Part 3: Dataset Preparation\n",
    "\n",
    "In this part we prepare the GSM8K dataset for training. GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. We will use the examples from this dataset to train our model in the reinforcement learning (RL) paradigm: the model will generate several sample probelem solutions, we will compare these solutions to the ground truth number from a GSM8K example and, if there's a match, we will provide a high reward to the RL algorithm (GRPO) which will update the model's weights so that the chance of getting the high reward next time is increased.\n",
    "\n",
    "We first load the dataset from Hugging Face and then format each example to include a system prompt and a user prompt. We also extract the expected answer from the dataset. Two helper functions are defined here:\n",
    "\n",
    "1. **`prepare_dataset`:** Loads and prepares the GSM8K dataset by creating a prompt that includes a system prompt (with the formatting instructions) and a user message (the question). It also extracts the answer from the dataset.\n",
    "2. **`build_prompt`:** Concatenates the list of message dictionaries into a single prompt string. This ensures consistency in how the prompt is constructed during both training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3a802-2ef2-4051-aafb-8f4c41e993b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(split=\"train\"):\n",
    "\n",
    "   data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "   formatted_data = []\n",
    "   for example in data:\n",
    "       # Convert list of messages to a single string prompt.\n",
    "       prompt_str = build_prompt([\n",
    "           {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "           {\"role\": \"user\", \"content\": example[\"question\"]}\n",
    "       ])\n",
    "       formatted_example = {\n",
    "           \"prompt\": prompt_str,  # Now a string rather than a list.\n",
    "           \"answer\": extract_answer_from_dataset(example[\"answer\"])\n",
    "       }\n",
    "       formatted_data.append(formatted_example)\n",
    "   return formatted_data\n",
    "\n",
    "def build_prompt(messages):\n",
    "\n",
    "   return \"\\n\".join([msg[\"content\"].strip() for msg in messages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d71f9b-1705-43a7-b97f-c16504639a67",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation Functions\n",
    "\n",
    "Evaluation is crucial to track the model's progress. In this part, we define functions that allow us to evaluate the model on a set of examples. The evaluation functions perform the following tasks:\n",
    "\n",
    "- **Tokenize the prompt and generate a response:** The model's output is generated given the tokenized prompt.\n",
    "- **Extract the predicted answer:** The answer is extracted from the generated response.\n",
    "- **Compare the predicted answer with the expected answer:** This comparison is done using exact matching as well as numeric equivalence checks.\n",
    "\n",
    "Two helper functions, `_extract_last_number` and `_extract_single_number`, are used to extract numbers from text. The main evaluation function, `evaluate_model`, uses these helpers to determine if the predicted answer is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3183f-c864-469b-9227-71a29ef864e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_number(text):\n",
    "\n",
    "   text = text.replace('$', '').replace('%', '')\n",
    "   pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$'\n",
    "   match = re.search(pattern, text)\n",
    "   return float(match.group(1)) if match else None\n",
    "\n",
    "def extract_single_number(text):\n",
    "\n",
    "   numbers = re.findall(r'-?\\d*\\.?\\d+', text)\n",
    "   return float(numbers[0]) if len(numbers) == 1 else None\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_examples, device):\n",
    "\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = len(eval_examples)\n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"EVALUATION ON\", total, \"EXAMPLES\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   for example in eval_examples:\n",
    "       # Get the prompt and expected answer\n",
    "       full_prompt = example[\"prompt\"]\n",
    "       expected = example[\"answer\"]\n",
    "\n",
    "       # Tokenize and generate response\n",
    "       inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "       with torch.no_grad():\n",
    "           outputs = model.generate(\n",
    "               inputs,\n",
    "               max_new_tokens=512,\n",
    "               temperature=0.7,\n",
    "               num_return_sequences=1,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               forced_eos_token_id=tokenizer.eos_token_id,\n",
    "               early_stopping=False,\n",
    "           )\n",
    "       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "       try:\n",
    "           # Extract answer and check correctness\n",
    "           predicted = extract_answer_from_model_output(response)\n",
    "\n",
    "           # Try different matching methods\n",
    "           if predicted == expected:  # Exact match\n",
    "               is_correct = True\n",
    "           else:\n",
    "               # Try single number matching\n",
    "               pred_num = extract_single_number(str(predicted))\n",
    "               exp_num = extract_single_number(str(expected))\n",
    "               if pred_num is not None and exp_num is not None and pred_num == exp_num:\n",
    "                   is_correct = True\n",
    "               else:\n",
    "                   # Try last number matching\n",
    "                   pred_num = extract_last_number(str(predicted))\n",
    "                   exp_num = extract_last_number(str(expected))\n",
    "                   is_correct = (pred_num is not None and exp_num is not None and\n",
    "                               pred_num == exp_num)\n",
    "\n",
    "           # Update counter for correct answers\n",
    "           if is_correct:\n",
    "               correct += 1\n",
    "\n",
    "           # Print evaluation details\n",
    "           print(\"\\nPrompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"\\nExpected Answer:\")\n",
    "           print(expected)\n",
    "           print(\"\\nExtracted Answer:\")\n",
    "           print(predicted)\n",
    "           print(\"\\nFull Generated Response:\")\n",
    "           print(response)\n",
    "           print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "           print(\"-\"*50)\n",
    "\n",
    "       except Exception as e:\n",
    "           print(\"\\nFailed to parse model output for prompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"Error:\", e)\n",
    "           print(\"-\"*50)\n",
    "\n",
    "   # Calculate and print final accuracy\n",
    "   accuracy = (correct / total) * 100\n",
    "   print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   # Return model to training mode\n",
    "   model.train()\n",
    "   return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c209b9-fc14-4c5d-a090-a9946bf5a405",
   "metadata": {},
   "source": [
    "## Part 5: Reward Functions\n",
    "\n",
    "In reinforcement learning, reward functions guide the training process by providing feedback on the model's output. In our pipeline, we define two reward functions:\n",
    "\n",
    "1. **`correctness_reward`:**  \n",
    "   This function assigns rewards based on whether the generated answer is correct. It compares the extracted answer from the model output with the expected answer, using both exact string matching and numeric equivalence checks. A exact match earns a higher reward (2.0), while a match based on numeric equivalence receives a smaller reward (1.5).\n",
    "   \n",
    "2. **`format_reward`:**  \n",
    "   This function encourages the model to adhere to the desired XML-like output format. It provides a small reward for the presence of the `<reasoning>`, `</reasoning>`, `<answer>`, and `</answer>` tags in the generated text. We use a relatively value of 0.05 for each of the four pieces because the model is already capable of using these tags from previous supervised finetuning step, so we give this small reward so that it doesn't forget to do that because of the RL updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017aa51f-e2d8-4263-9539-f9b5eb58f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward(prompts, completions, answer, **kwargs):\n",
    "\n",
    "   responses = [completion[0]['content'] for completion in completions]\n",
    "   extracted = [extract_answer_from_model_output(r) for r in responses]\n",
    "   rewards = []\n",
    "   for r, a in zip(extracted, answer):\n",
    "       if r == a:  # Exact match case\n",
    "           rewards.append(2.0)\n",
    "       else:\n",
    "           # Try numeric equivalence\n",
    "           r_num = extract_single_number(str(r))\n",
    "           a_num = extract_single_number(str(a))\n",
    "           if r_num is not None and a_num is not None and r_num == a_num:\n",
    "               rewards.append(1.5)\n",
    "           else:\n",
    "               rewards.append(0.0)\n",
    "   # Log completion lengths\n",
    "   completion_lengths = [len(response.split()) for response in responses]\n",
    "   return rewards\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "\n",
    "   responses = [completion[0]['content'] for completion in completions]\n",
    "   rewards = []\n",
    "   format_scores = []\n",
    "   for response in responses:\n",
    "       score = 0.0\n",
    "       if \"<reasoning>\" in response: score += 0.2\n",
    "       if \"</reasoning>\" in response: score += 0.2\n",
    "       if \"<answer>\" in response: score += 0.2\n",
    "       if \"</answer>\" in response: score += 0.2\n",
    "       rewards.append(score)\n",
    "       format_scores.append(score)\n",
    "   return rewards\n",
    "\n",
    "def combined_reward(prompts, completions, answer):\n",
    "\n",
    "   # Get individual rewards\n",
    "   correctness_scores = correctness_reward(prompts=prompts, completions=completions, answer=answer)\n",
    "   format_scores = format_reward(completions=completions)\n",
    "\n",
    "   # Combine rewards - correctness is weighted more heavily\n",
    "   combined_rewards = []\n",
    "   for c_score, f_score in zip(correctness_scores, format_scores):\n",
    "       # Correctness score range: 0.0 to 2.0\n",
    "       # Format score range: 0.0 to 0.8\n",
    "       # Total range: 0.0 to 2.8\n",
    "       combined_rewards.append(c_score + f_score)\n",
    "\n",
    "   return combined_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64bcdc-82ef-4907-8fe1-978bb0ef60b4",
   "metadata": {},
   "source": [
    "## Part 6: DataParallel GRPO From Scratch\n",
    "\n",
    "In this section, we implement all the building blocks of the GRPO algorithm from scratch. The implementation assumes that the machine running the code has at least 2 GPUs. We use PyTorch's `DataParallel` API to distribute the policy model across the GPU cores, one copy of the model per GPU core. The batch is split between the GPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aee346-5d8c-4659-a1ff-23d6b54b2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_log_softmax(logits, input_ids):\n",
    "\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def compute_log_probs(model, input_ids, attention_mask, logits_to_keep):\n",
    "\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :]\n",
    "    input_ids = input_ids[:, -logits_to_keep:]\n",
    "    logits = logits[:, -logits_to_keep:, :]\n",
    "    return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "def create_completion_mask(completion_ids, eos_token_id):\n",
    "\n",
    "    is_eos = completion_ids == eos_token_id\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"].to(device)\n",
    "    prompt_mask = inputs[\"attention_mask\"].to(device)\n",
    "    print(f\"Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\")\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
    "    outputs = model.generate(\n",
    "        prompt_ids,\n",
    "        attention_mask=prompt_mask,\n",
    "        max_new_tokens=max_completion_length,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    print(f\"Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\")\n",
    "    completion_ids = outputs[:, prompt_length:]\n",
    "    completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id)\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask\n",
    "\n",
    "def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompts = [sample[\"prompt\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
    "    answers = [sample[\"answer\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
    "    with torch.no_grad():\n",
    "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
    "            model, tokenizer, prompts, num_generations, max_completion_length\n",
    "        )\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "        old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "        ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep)\n",
    "    formatted_completions = [[{'content': tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids]\n",
    "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]\n",
    "    repeated_answers = [a for a in answers for _ in range(num_generations)]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"old_log_probs\": old_log_probs,\n",
    "        \"ref_log_probs\": ref_log_probs,\n",
    "        \"formatted_completions\": formatted_completions,\n",
    "        \"repeated_prompts\": repeated_prompts,\n",
    "        \"repeated_answers\": repeated_answers,\n",
    "        \"logits_to_keep\": logits_to_keep,\n",
    "        \"batch_size\": len(prompts),\n",
    "        \"num_generations\": num_generations\n",
    "    }\n",
    "\n",
    "def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = rollout_data[\"input_ids\"]\n",
    "    attention_mask = rollout_data[\"attention_mask\"]\n",
    "    completion_mask = rollout_data[\"completion_mask\"]\n",
    "    logits_to_keep = rollout_data[\"logits_to_keep\"]\n",
    "    old_log_probs = rollout_data[\"old_log_probs\"]\n",
    "    ref_log_probs = rollout_data[\"ref_log_probs\"]\n",
    "    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "    ratio = torch.exp(token_log_probs - old_log_probs)\n",
    "    rewards = torch.tensor(\n",
    "        reward_function(prompts=rollout_data[\"repeated_prompts\"], completions=rollout_data[\"formatted_completions\"], answer=rollout_data[\"repeated_answers\"]),\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "    #print(f\"Rewards: {rewards}\")  # Debug rewards\n",
    "    batch_size = rollout_data[\"batch_size\"]\n",
    "    num_generations = rollout_data[\"num_generations\"]\n",
    "    rewards = rewards.view(batch_size, num_generations)\n",
    "    avg_reward = rewards.mean().item()\n",
    "    print(\"Average Reward:\", avg_reward)\n",
    "    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)\n",
    "    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)\n",
    "    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate_loss = torch.min(surr1, surr2)\n",
    "    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1\n",
    "    per_token_loss = surrogate_loss - beta * kl\n",
    "    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "    return loss, avg_reward\n",
    "\n",
    "def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4,\n",
    "                              num_generations=4, max_completion_length=128, beta=0.1,\n",
    "                              learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None):\n",
    "\n",
    "    assert device_ids is not None and len(device_ids) > 1, \"This code needs at least 2 GPU cores to run!\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Wrap model with DataParallel if multiple GPUs are available.\n",
    "\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    print(f\"Model wrapped with DataParallel across GPUs: {device_ids}\")\n",
    "\n",
    "    # Outer loop: iterative GRPO updates.\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "\n",
    "        # Create a reference model (deep copy) and set it to eval mode.\n",
    "        ref_model = copy.deepcopy(model.module)\n",
    "        ref_model.eval()\n",
    "        for param in ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Reference model created.\")\n",
    "\n",
    "        # Reinitialize the optimizer for this iteration.\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "\n",
    "        # Inner loop: your original training steps.\n",
    "        for step in range(num_steps):\n",
    "            batch_samples = random.sample(train_data, batch_size)\n",
    "            with torch.no_grad():\n",
    "                rollout_data = generate_rollout_data(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    tokenizer,\n",
    "                    batch_samples,\n",
    "                    num_generations,\n",
    "                    max_completion_length\n",
    "                )\n",
    "            for grpo_iter in range(mu):\n",
    "                loss, avg_reward = grpo_loss(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    rollout_data,\n",
    "                    tokenizer,\n",
    "                    reward_function,\n",
    "                    beta=beta,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "                # Log to wandb\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"average_reward\": avg_reward,\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"step\": step + 1,\n",
    "                    \"grpo_iter\": grpo_iter + 1\n",
    "                })\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \"\n",
    "                      f\"GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\")\n",
    "                #for i in range(torch.cuda.device_count()):\n",
    "                #    print(f\"GPU {i} Usage: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MiB, \"\n",
    "                #          f\"Utilization: {torch.cuda.utilization(i)}%\")\n",
    "                # Uncomment to see the GPU utilization stats\n",
    "    return model.module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f5ffe-6c2b-459c-981a-3624e086feb5",
   "metadata": {},
   "source": [
    "## Part 7: Training Setup and Execution\n",
    "\n",
    "In this section, we put together all components to set up and run the training. We begin by loading the pre-trained model and tokenizer, prepare evaluation data, and then do reinforcement learning (RL) fine-tuning using the our own `train_with_grpo` we implemented from scratch above.\n",
    "\n",
    "Key steps include:\n",
    "\n",
    "- **Model and Tokenizer Initialization:**  \n",
    "  The model `\"Qwen/Qwen2.5-1.5B-Instruct\"` is loaded with optimized settings (using `torch.bfloat16` and FlashAttention2). The tokenizer is also loaded, and its padding token is set to the end-of-sequence token. Loading a model with `torch.bfloat16` converts its parameters to use 16 bits instead of 32 bits per number, which cuts the model's memory usage in half and can make training faster on modern GPUs.\n",
    "  \n",
    "- **Initial Evaluation:**  \n",
    "  Before fine-tuning, the model is evaluated on a few examples to establish a baseline performance.\n",
    "    \n",
    "- **Reinforcement Learning Fine-Tuning (RL):**  \n",
    "  The training function `train_with_grpo` implementing GRPO from scratch is configured with the appropriate training arguments and reward functions. The RL training then proceeds on the remaining training data.\n",
    "  \n",
    "- **Final Evaluation and Model Saving:**  \n",
    "  After RL fine-tuning, the model is evaluated again, and the final model is saved.\n",
    "\n",
    "In the code below:\n",
    "  \n",
    "- The device is determined (GPU if available, otherwise CPU).\n",
    "- The pre-trained Qwen2.5-1.5B-Instruct model and tokenizer are loaded. The tokenizer's pad token is set to the eos_token.\n",
    "- A small subset of the dataset is reserved for evaluation to provide a baseline.\n",
    "- The model is optimized for memory efficiency by enabling gradient checkpointing and disabling KV caching.\n",
    "- **Step 1:** The model is evaluated before fine-tuning to establish a baseline accuracy.\n",
    "- **Step 2:** Reinforcement learning fine-tuning is performed using the `train_with_grpo` function with our defined reward functions (`format_reward` and `correctness_reward`, combined into `combined_reward`). The model is trained using a multi-GPU.\n",
    "- **Step 3:** The final, fine-tuned model and tokenizer are saved to disk.\n",
    "\n",
    "We used the following hyperparameters for our GRPO training pipeline:\n",
    "\n",
    "### **Training Configuration**\n",
    "\n",
    "These parameters configure the reinforcement learning fine-tuning run using the GRPO algorithm. We set them as follows:\n",
    "\n",
    "- **num_iterations=1**  \n",
    "  The number of outer iterations where a new reference model is created from the current policy model. One iteration is one pass over the entire dataset.\n",
    "\n",
    "- **num_steps=500**  \n",
    "  The training loop will perform a maximum of 500 steps, each processing a batch of examples.\n",
    "\n",
    "- **batch_size=7**  \n",
    "  Each step processes 7 examples per batch which, in the case of 8 GPUs, puts 1 example at each GPU. One GPU (0) is used as the master by `DataParallel` for aggregating gradients and gathering outputs.\n",
    "\n",
    "- **num_generations=14**  \n",
    "  For every prompt in the training data, the trainer will generate 14 different completions. These multiple generations are used to compute a relative advantage (or reward signal) that guides the RL update. Reduce this number if you have GPUs with less VRAM.\n",
    "\n",
    "- **max_completion_length=400**  \n",
    "  When generating completions (the \"response\" portion of the sequence), the generation is capped at 400 tokens. This limits the length of the outputs produced by the model during the RL phase. Reduce this number if you have GPUs with less VRAM.\n",
    "\n",
    "- **beta=0.04**  \n",
    "  The coefficient for the KL divergence penalty in the GRPO loss function. This controls how much the model is allowed to diverge from the reference model.\n",
    "\n",
    "- **learning_rate=5e-6**  \n",
    "  The learning rate for RL finetuning. A relatively low learning rate is used for stable policy updates.\n",
    "\n",
    "- **mu=1**  \n",
    "  The number of policy updates performed for each batch of rollout data. In our case, we perform just one update per batch.\n",
    "\n",
    "- **epsilon=0.1**  \n",
    "  The clipping parameter for the PPO component of GRPO. This prevents the policy from changing too drastically in a single update.\n",
    "\n",
    "The model is evaluated both before and after fine-tuning to measure the improvement in accuracy. Finally, the fine-tuned model is saved to the \"grpo_finetuned_model\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7905e-8729-4368-8854-1e13c686f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model_memory(model):\n",
    "    model.train()\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # First ensure inputs will require gradients\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "    # Then enable gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Main execution\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using primary device: {device}\")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "output_dir = \"math_solver_model\"\n",
    "\n",
    "print(\"Downloading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model downloaded\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Detected {num_gpus} GPUs\")\n",
    "device_ids = list(range(num_gpus)) if num_gpus > 1 else None\n",
    "\n",
    "all_data = prepare_dataset(\"train\")\n",
    "random.shuffle(all_data)\n",
    "size_of_eval_data = 30 # change to a smaller value to save time or to a larger number for a more reliable estimate\n",
    "eval_data = all_data[:size_of_eval_data]\n",
    "train_data = all_data[size_of_eval_data:]\n",
    "\n",
    "print(\"\\nInitial model evaluation before finetuning:\")\n",
    "pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%\")\n",
    "\n",
    "model = optimize_model_memory(model)\n",
    "\n",
    "print(\"\\nStarting RL fine-tuning using GRPO...\")\n",
    "# This config was tested on a 8xA100 node, where each A100 is has 80GB of VRAM\n",
    "training_config = {\n",
    "    'num_iterations': 1,\n",
    "    'num_steps': 500,\n",
    "    'batch_size': 7, # reduce if you have fewer GPUs\n",
    "    'num_generations': 12, # reduce if you have GPUs with less VRAM\n",
    "    'max_completion_length': 400, # reduce if you have GPUs with less VRAM\n",
    "    'beta': 0.04,\n",
    "    'learning_rate': 5e-6,\n",
    "    'mu': 1,\n",
    "    'epsilon': 0.1\n",
    "}\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=os.environ[\"WANDB_PROJECT\"], reinit=True)\n",
    "print(\"Weights & Biases initialized.\")\n",
    "\n",
    "model = train_with_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_data=train_data,\n",
    "    reward_function=combined_reward,\n",
    "    device_ids=device_ids,\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Training completed and wandb run finished.\")\n",
    "\n",
    "print(\"\\nFinal model evaluation after GRPO RL fine-tuning:\")\n",
    "post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nSaving GRPO fine-tuned model...\")\n",
    "model.save_pretrained(\"grpo_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"grpo_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cba5fd-a554-47b7-89b7-97353743e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Step 4. LOAD AND TEST MODEL  #\n",
    "###########################\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Determine the device: use GPU if available, else fallback to CPU.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the saved model and tokenizer\n",
    "    saved_model_path = \"grpo_finetuned_model\"\n",
    "\n",
    "\n",
    "    # Load the model\n",
    "    loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "        saved_model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "    loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n",
    "\n",
    "    # Define test prompts\n",
    "    prompts_to_test = [\n",
    "        \"How much is 1+1?\",\n",
    "        \"I have 3 apples, my friend eats one and I give 2 to my sister, how many apples do I have now?\",\n",
    "        \"Solve the equation 6x + 4 = 40\"\n",
    "    ]\n",
    "\n",
    "    # Test each prompt\n",
    "    for prompt in prompts_to_test:\n",
    "        # Prepare the prompt using the same format as during training\n",
    "        test_messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        test_prompt = build_prompt(test_messages)\n",
    "\n",
    "        # Tokenize the prompt and generate a response\n",
    "        test_input_ids = loaded_tokenizer.encode(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate response with similar parameters to those used in training\n",
    "        with torch.no_grad():\n",
    "            test_output_ids = loaded_model.generate(\n",
    "                test_input_ids,\n",
    "                max_new_tokens=400,\n",
    "                temperature=0.7,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=loaded_tokenizer.pad_token_id,\n",
    "                eos_token_id=loaded_tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                early_stopping=False\n",
    "            )\n",
    "\n",
    "        test_response = loaded_tokenizer.decode(test_output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Print the test prompt and the model's response\n",
    "        print(\"\\nTest Prompt:\")\n",
    "        print(test_prompt)\n",
    "        print(\"\\nModel Response:\")\n",
    "        print(test_response)\n",
    "\n",
    "        # Extract and display the answer part for easier evaluation\n",
    "        try:\n",
    "            extracted_answer = extract_answer_from_model_output(test_response)\n",
    "            print(\"\\nExtracted Answer:\")\n",
    "            print(extracted_answer)\n",
    "            print(\"-\" * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to extract answer: {e}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd306a-6669-471d-a8b3-84ac00d1bf57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
